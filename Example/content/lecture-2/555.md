### Глава 4. Анализ сложности алгоритмов

Нашей целью теперь будет научиться оценивать время работы программы, а лучше даже алгоритма. Допустим, программа получает на вход массив длины N. Тогда нас интересует $T(N)$ — максимальное время работы этой программы на входе длины N (как функция от N). Различных входов длины N много, но мы выбираем максимум, и получаем время работы «в худшем случае».

Хочется оценивать скорость роста $T(N)$:

* Для больших N (на маленьких и так всё работает достаточно быстро).
* С точностью до константы (константа зависит от «аппаратуры», «компилятора» и пр.).
* Оценивать будем сверху (обычно нам важно понять, что программа укладывается в отведённое время).

Асимптотическая оценка сверху записывается так: $T(n) = O(g(n))$, где $g(n)$ — какая-то положительная функция, показывающая скорость роста. Например, $T(n) = O(n^2)$ означает, что на больших n время работы не превышает $Cn^2$ (для некоторой положительной константы C). Символ $O(...)$ называется «O-большое» («big-O»).

Строгое определение: $T(n) = O(g(n))$, если существуют $n_0, C$, такие что: для всех $n > n_0$ верно: $T(n) < Cg(n)$.
Альтернативные определения: $T(n)/g(n) \le C$ при $n > n_0$.
Кроме того, можно определить через верхний предел: $C_0 = \limsup_{n\to\infty} (T(n)/g(n))$.
Тогда:

* если $C_0$ конечное, то верно $T(n) = O(g(n))$;
* если $C_0$ бесконечное, то $T(n) = O(g(n))$ неверно.

**Замечания:**

* Оценки O-большое даются сверху, т.е. если $T(n) = O(n^2)$, то и $T(n) = O(n^3)$ тоже верно. Для точной оценки есть похожий символ $\Theta$-большое (тета-большое).
* Кроме времени работы, также имеет смысл оценивать требуемую память $M(n)$.
* Время работы может сильно зависеть не только от количества элементов, но и от других параметров.
* Обычно функция $g(n)$ положительна и отделена от нуля, поэтому в определении можно полагать $n_0 = 0$.

**Примеры:**

* сумма чисел в массиве длины N: $T(N) = O(N)$;
* проверка числа N на простоту: $T(N) = O(\sqrt{N})$ (если перебирать делители до $\sqrt{N}$);
* k вложенных циклов, каждый делает порядка N итераций: $T(N) = O(N^k)$.

Обычно вы оцениваем количество «простых действий», которые сделает программа, а не время её работы. Простыми считаются такие действия:

* обращение к 32-битному (или 64-битному) значению в памяти;
* арифметические или логические операции над 32-битными (или 64-битными) числами;
* срабатывание условного перехода.

Примеры действий, которые простыми не являются:

* проверка 32-битного числа на простоту;
* вычисление суммы 1024-битных чисел;
* сравнение строк.

Тактовая частота современного компьютера имеет порядок $1 \text{GHz}$, то есть в секунду укладывается порядка $10^9$ тактов, а это значит, что выполняется примерно $10^8$ операций в секунду.

В таблице приведены ориентировочные данные о том, для какого характерного размера входных данных программа с известной асимптотикой будет успевать отработать за секунду.

* **T(N) = O(1), O(log N):** N произвольное (всё пройдёт, рост очень медленный)
* **T(N) = O(√N):** N ≤ 10¹⁶
* **T(N) = O(N):** N ≤ 10⁸
* **T(N) = O(N log N):** N ≤ 10⁶
* **T(N) = O(N√N):** N ≤ 10⁵
* **T(N) = O(N²):** N ≤ 10⁴
* **T(N) = O(N³):** N ≤ 500
* **T(N) = O(N⁴):** N ≤ 100
* **T(N) = O(2ᴺ):** N ≤ 26
* **T(N) = O(N!):** N ≤ 13